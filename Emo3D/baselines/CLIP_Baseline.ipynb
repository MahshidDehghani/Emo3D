{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JekQHusU4jHF"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install huggingface_hub\n",
        "!pip3 install torch torchvision\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5z_IbI-cfA5"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "# from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwJVcX99cZZ8"
      },
      "source": [
        "# Download and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxSnx01qjkiq"
      },
      "outputs": [],
      "source": [
        "login(token=\"\")\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yp6V2cl4rAS"
      },
      "outputs": [],
      "source": [
        "files_to_download = [\n",
        "    \"data/emo3d_data.parquet\",\n",
        "    \"data/train_data.csv\",\n",
        "    \"data/val_data.csv\",\n",
        "    \"data/test_data.csv\",\n",
        "    \"data/anger_images.zip\",\n",
        "    \"data/contempt_images.zip\",\n",
        "    \"data/disgust_images.zip\",\n",
        "    \"data/fear_images.zip\",\n",
        "    \"data/sadness_images.zip\",\n",
        "    # # \"data/primitive_emotions.csv\",\n",
        "    # # \"data/primitive_emotions.zip\",\n",
        "    \"data/prompt1_images_0_1000.zip\",\n",
        "    \"data/prompt1_images_1000_2000.zip\",\n",
        "    \"data/prompt1_images_2000_3000.zip\",\n",
        "    \"data/prompt1_images_3000_4000.zip\",\n",
        "    \"data/prompt1_images_4000_5000.zip\",\n",
        "    \"data/prompt1_images_5000_6000.zip\",\n",
        "    \"data/prompt1_images_6000_7000.zip\",\n",
        "    \"data/prompt1_images_7000_9000.zip\",\n",
        "    \"data/prompt1_images_9000_10000.zip\",\n",
        "    \"data/prompt2_images_0_1000.zip\",\n",
        "    \"data/prompt2_images_1000_2000.zip\",\n",
        "    \"data/prompt2_images_2000_3000.zip\",\n",
        "    \"data/prompt2_images_3000_4000.zip\",\n",
        "    \"data/prompt2_images_4000_5000.zip\",\n",
        "    \"data/prompt2_images_5000_6000.zip\",\n",
        "]\n",
        "\n",
        "\n",
        "repo_id = \"llm-lab/Emo3D\"\n",
        "local_dir = \"/content\"\n",
        "# Download all files\n",
        "for file_path in files_to_download:\n",
        "    print(f\"Downloading {file_path}...\")\n",
        "    hf_hub_download(repo_type='dataset', repo_id=repo_id, filename=file_path, local_dir=local_dir)\n",
        "    print(f\"{file_path} downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8sddutdhCD8"
      },
      "outputs": [],
      "source": [
        "for file_path in files_to_download:\n",
        "    local_file_path = os.path.join(local_dir, file_path)  # Full path to the file\n",
        "    print(local_file_path)\n",
        "    if file_path.endswith(\".zip\"):  # Check if it's a zip file\n",
        "        print(f\"Unzipping {file_path}...\")\n",
        "\n",
        "        unzip_dir = file_path[:-4]\n",
        "        with zipfile.ZipFile(local_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzip_dir)  # Extract to the same directory\n",
        "        os.remove(local_file_path)\n",
        "        print(f\"{file_path} extracted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUfMiPfrqCXK"
      },
      "outputs": [],
      "source": [
        "blendshapes_names = ['_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft', 'browOuterUpRight', 'cheekPuff',\n",
        " 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft', 'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft',\n",
        " 'eyeLookInRight', 'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft', 'eyeSquintRight',\n",
        " 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen', 'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight',\n",
        " 'mouthFrownLeft', 'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight', 'mouthPressLeft',\n",
        " 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower', 'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft',\n",
        " 'mouthSmileRight', 'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight', 'noseSneerLeft', 'noseSneerRight']\n",
        "\n",
        "\n",
        "bs_mirror_idx = []\n",
        "for i, name in enumerate(blendshapes_names):\n",
        "    if \"Left\" in name:\n",
        "        right_name = name.replace(\"Left\", \"Right\")\n",
        "        right_index = blendshapes_names.index(right_name)\n",
        "        bs_mirror_idx.append(right_index)\n",
        "    elif \"Right\" in name:\n",
        "        left_name = name.replace(\"Right\", \"Left\")\n",
        "        left_index = blendshapes_names.index(left_name)\n",
        "        bs_mirror_idx.append(left_index)\n",
        "    else:\n",
        "        bs_mirror_idx.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6he0dZ5TyOs"
      },
      "outputs": [],
      "source": [
        "replacements = {\n",
        "        \"left\": \"right\",\n",
        "        \"right\": \"left\",\n",
        "        \"Left\": \"Right\",\n",
        "        \"Right\": \"Left\"\n",
        "    }\n",
        "\n",
        "regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, replacements.keys())))\n",
        "def mirror_text(text):\n",
        "    return regex.sub(lambda mo: replacements[mo.group()], text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RafQt9Acy-s"
      },
      "source": [
        "# Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188ZRSGWdC5p"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB_Ei0-pDT8a"
      },
      "outputs": [],
      "source": [
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW3a7m57tJZI"
      },
      "outputs": [],
      "source": [
        "image_dir = \"/content/data\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, augmentation=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.data = []\n",
        "\n",
        "        # Loop over rows to create the pairs\n",
        "        text_cols = ['text_1', 'text_2', 'text_3']\n",
        "        image_cols = ['img_1', 'img_2', 'img_3', 'img_4']\n",
        "        bs_cols = ['blenshape_score_1', 'blenshape_score_2', 'blenshape_score_3', 'blenshape_score_4']\n",
        "\n",
        "        for i in tqdm(range(len(self.dataframe))):\n",
        "            for j, bs_col in enumerate(bs_cols):\n",
        "                if not pd.isna(dataframe[bs_col].iloc[i]):\n",
        "                    blendshape_score = dataframe[bs_col].iloc[i]\n",
        "                    bs_arr = np.array(blendshape_score.strip(\"[]\").split(), dtype=float)\n",
        "                    mirror_bs_arr = self.mirror_blenshape_score(bs_arr)\n",
        "                    # Text embeddings\n",
        "                    for text_col in text_cols:\n",
        "                        text = dataframe[text_col].iloc[i]\n",
        "                        text_embedding = self.get_text_embedding(text)\n",
        "                        self.data.append((text_embedding, torch.from_numpy(bs_arr[1:]).to(device)))\n",
        "\n",
        "                        if augmentation == True:\n",
        "                            # Mirror blendshape score\n",
        "                            mirror_text_embedding = self.get_text_embedding(mirror_text(text))\n",
        "                            self.data.append((mirror_text_embedding, torch.from_numpy(mirror_bs_arr[1:]).to(device)))\n",
        "\n",
        "                    # Image embeddings\n",
        "                    img_col = image_cols[j]\n",
        "                    if not pd.isna(dataframe[img_col].iloc[i]):\n",
        "                        image_path = os.path.join(image_dir, dataframe[img_col].iloc[i])\n",
        "                        image_embedding = self.get_image_embedding(image_path)\n",
        "                        self.data.append((image_embedding, torch.from_numpy(bs_arr[1:]).to(device)))\n",
        "\n",
        "                        if augmentation == True:\n",
        "                            # Mirror blendshape score\n",
        "                            mirror_image_embedding = self.gent_mirror_image_embedding(image_path)\n",
        "                            self.data.append((mirror_image_embedding, torch.from_numpy(mirror_bs_arr[1:]).to(device)))\n",
        "\n",
        "\n",
        "    def get_text_embedding(self, text):\n",
        "        inputs = clip.tokenize(text).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = clip_model.encode_text(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def get_image_embedding(self, image_path):\n",
        "        inputs = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = clip_model.encode_image(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def gent_mirror_image_embedding(self, image_path):\n",
        "        inputs = preprocess(Image.open(image_path).transpose(Image.FLIP_LEFT_RIGHT)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = clip_model.encode_image(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def mirror_blenshape_score(self, bs):\n",
        "        \"\"\"Mirrors the blendshape array based on predefined symmetry indices.\"\"\"\n",
        "        return bs[bs_mirror_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding, blendshape_score = self.data[idx]\n",
        "        return embedding, blendshape_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlTs0psUJDFu"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/data/train_data.csv\")\n",
        "val_df = pd.read_csv(\"/content/data/val_data.csv\")\n",
        "test_df = pd.read_csv(\"/content/data/test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrLhOwX26zVb"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_df)\n",
        "val_dataset = CustomDataset(val_df, augmentation=False)\n",
        "test_dataset = CustomDataset(test_df, augmentation=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExhoKPbgeXzR"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKGb_F3IeXJv"
      },
      "outputs": [],
      "source": [
        "class ClipMLP(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim1=256, hidden_dim2=128, output_dim=51):\n",
        "        super(ClipMLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim2, output_dim),\n",
        "            nn.Sigmoid()  # Using sigmoid to ensure outputs between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kmxG_KpeTAJ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9D8vPJhGDx9",
        "outputId": "a4afc641-20cd-4f0c-cfb4-7a7350a2bd9d"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50):\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for embeddings, blendshape_scores in train_loader:\n",
        "            # Move data to device\n",
        "            embeddings = embeddings.to(device).float()\n",
        "            blendshape_scores = blendshape_scores.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(embeddings).squeeze()\n",
        "            loss = criterion(outputs, blendshape_scores)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        \n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for embeddings, blendshape_scores in val_loader:\n",
        "                # Move data to device\n",
        "                embeddings = embeddings.to(device).float()\n",
        "                blendshape_scores = blendshape_scores.to(device).float()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(embeddings).squeeze()\n",
        "                loss = criterion(outputs, blendshape_scores)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print epoch statistics\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {train_loss:.4f}, '\n",
        "              f'Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss <= best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model\n",
        "model = ClipMLP().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g_7xMr3eeJI"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogeg8C6sPv9P",
        "outputId": "896ce5bb-f527-4919-a2fa-d968d5fbbbce"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "trained_model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for embeddings, blendshape_scores in test_loader:\n",
        "        embeddings = embeddings.to(device).float()\n",
        "        blendshape_scores = blendshape_scores.to(device).float()\n",
        "\n",
        "        outputs = trained_model(embeddings).squeeze()\n",
        "        loss = criterion(outputs, blendshape_scores)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "torch.save(trained_model.state_dict(), 'final_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBtFSgfGRWzj",
        "outputId": "16c02609-f277-49df-a848-663f88c71a99"
      },
      "outputs": [],
      "source": [
        "best_model = ClipMLP().to(device)\n",
        "best_model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
        "best_model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for embeddings, blendshape_scores in test_loader:\n",
        "        embeddings = embeddings.to(device).float()\n",
        "        blendshape_scores = blendshape_scores.to(device).float()\n",
        "\n",
        "        outputs = best_model(embeddings).squeeze()\n",
        "        loss = criterion(outputs, blendshape_scores)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "print(f'Test Loss: {test_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The face is expressing a sense of overwhelming joy, radiating happiness and contentment.\"\n",
        "inputs = clip.tokenize(text).to(device)\n",
        "with torch.no_grad():\n",
        "    embedding = clip_model.encode_text(inputs).float()\n",
        "bs_score_pred = best_model(embedding).cpu().detach().numpy()\n",
        "print(np.around(bs_score_pred, decimals=2, out=None))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
